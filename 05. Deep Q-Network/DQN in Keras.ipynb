{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP Q-NETWORK ALGORITHM\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will implement the Deep Q-Networks algorithm in Keras.\n",
    "\n",
    "<img width=\"900px\" src=\"./assets/dqn_intro.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 1. Import the Packages\n",
    "\n",
    "---\n",
    "\n",
    "Below let's import all the libraries that we are going to use in this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 2. Deep Q-Network (DQN)\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we will build the Deep Q-Network (DQN) algorithm in Keras. Later on we will add the extensions and observe the difference in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network (DQN) Agent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init function\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"\n",
    "        Init function.\n",
    "        \n",
    "        ARGUMENTS\n",
    "        ========================\n",
    "            - state_size: Size of the state space\n",
    "            - action-size: Size of action space\n",
    "        \"\"\"\n",
    "        # Initialize state size object\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Initialize action size object\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Initialize memory object\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        \n",
    "        # Initialize discount rate object\n",
    "        self.gamma = 0.9            \n",
    "        \n",
    "        # Initialize exploration rate object\n",
    "        self.epsilon = 1.0         \n",
    "        \n",
    "        # Initialize minimal exploration rate (epsilon-greedy) object\n",
    "        self.epsilon_min = 0.1      \n",
    "        \n",
    "        # Initialize decay rate for epsilon object\n",
    "        self.epsilon_decay = 0.995  \n",
    "        \n",
    "        # Initialize number of steps until updating the target network object\n",
    "        self.update_rate = 1000     \n",
    "        \n",
    "        # Construct DQN models\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.model.summary()\n",
    "\n",
    "        \n",
    "    # Model architecture\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Building a neural networks architecture.\n",
    "        \"\"\"\n",
    "        # Initialize the model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Conv layer 1\n",
    "        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # Conv layer 2\n",
    "        model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # Conv layer 3\n",
    "        model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # Flatten the neurons\n",
    "        model.add(Flatten())\n",
    "\n",
    "        # FCL layer 1\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        \n",
    "        return model\n",
    "\n",
    "    \n",
    "    # Function for storing the experience in the replay memory\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store the experience in the replay memory.\n",
    "        \n",
    "        ARGUMENTS\n",
    "        ========================\n",
    "            - state: Current state\n",
    "            - action: Action \n",
    "            - reward: Reward\n",
    "            - next_state: Next state\n",
    "            - done: Indicates if it's a terminal state or not.\n",
    "        \"\"\"\n",
    "        # Append the (S, A, R, S', done) into the memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    # Function for choosing action based on a epsilon-greedy policy\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Choose action based on a epsilon-greedy policy.\n",
    "        \n",
    "        ARGUMENTS\n",
    "        ========================\n",
    "            - state: Current state\n",
    "        \"\"\"\n",
    "        # Random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action_to_take = random.randrange(self.action_size)\n",
    "\n",
    "        # Greedy action\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            action_to_take = np.argmax(act_values[0])\n",
    "        \n",
    "        return action_to_take\n",
    "\n",
    "\n",
    "    # Function for randomely selecting experiences (in the replay memory) and training on them\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomely selecting experiences (in the replay memory) and training on them.\n",
    "        \n",
    "        ARGUMENTS\n",
    "        ========================\n",
    "            - batch_size: Batch size\n",
    "        \"\"\"\n",
    "        # Get a random batch\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Iterate through the (S, A, R, S')\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            # If terminal state\n",
    "            if not done:\n",
    "                \n",
    "                # Calculate the target\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)))\n",
    "                \n",
    "            # If not terminal state\n",
    "            else:\n",
    "                \n",
    "                # Assign reward to our target\n",
    "                target = reward\n",
    "                \n",
    "            ### Construct the target vector\n",
    "            \n",
    "            # 1. Output the Q-value predictions\n",
    "            target_f = self.model.predict(state)\n",
    "            \n",
    "            # 2. Update the action values with the target\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            # 3. Use vectors in the objective computation\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        # Decay epsilon (if it's larger than epsilon_min)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "            \n",
    "    # Function for updating the target model parameters\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Update the target model parameters to the current model parameters\n",
    "        \"\"\"\n",
    "        # Update the target model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "            \n",
    "\n",
    "    # Load a saved model\n",
    "    def load(self, name):\n",
    "        \"\"\"\n",
    "        Load the saved model\n",
    "        \"\"\"\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "        \n",
    "    # Save parameters of a trained model\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Save the parameters of a trained model\n",
    "        \"\"\"\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing the frame\n",
    "def process_frame(frame):\n",
    "    \"\"\"\n",
    "    Preprocessing the frame.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ========================\n",
    "        - frame: One frame of the video to preprocess.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    mspacman_color = np.array([210, 164, 74]).mean()\n",
    "    \n",
    "    # Crop and downsize\n",
    "    img = frame[1:176:2, ::2]    \n",
    "    \n",
    "    # Convert to greyscale\n",
    "    img = img.mean(axis=2)       \n",
    "    \n",
    "    # Improve contrast by making pacman white\n",
    "    img[img==mspacman_color] = 0 \n",
    "    \n",
    "    # Normalize from -1 to 1\n",
    "    img = (img - 128) / 128 - 1  \n",
    "    \n",
    "    return np.expand_dims(img.reshape(88, 80, 1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for blending the images\n",
    "def blend_images(images, blend):\n",
    "    \"\"\"\n",
    "    Blend images.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    ========================\n",
    "        - images\n",
    "        - blend\n",
    "    \"\"\"\n",
    "    avg_image = np.expand_dims(np.zeros((88, 80, 1), np.float64), axis=0)\n",
    "\n",
    "    for image in images:\n",
    "        avg_image += image\n",
    "        \n",
    "    if len(images) < blend:\n",
    "        return avg_image / len(images)\n",
    "    \n",
    "    else:\n",
    "        return avg_image / blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make('SpaceInvaders-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "# State size\n",
    "state_size = (88, 80, 1)\n",
    "\n",
    "# Action size\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Number of episodes\n",
    "episodes = 500\n",
    "\n",
    "# Batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Waits for 90 actions before the episode begins\n",
    "skip_start = 90  \n",
    "\n",
    "# Counter for total number of steps taken\n",
    "total_time = 0   \n",
    "\n",
    "# Used to compute avg reward over time\n",
    "all_rewards = 0  \n",
    "\n",
    "# Number of images to blend\n",
    "blend = 4       \n",
    "\n",
    "# Terminal state\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 22, 20, 32)        2080      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 22, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 10, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 11, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 11, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 11, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 7040)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               3604992   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 3,679,910\n",
      "Trainable params: 3,679,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DQN agent\n",
    "agent = DQN_Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through episodes\n",
    "for e in range(episodes):\n",
    "    \n",
    "    # Initialize the total reward\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Initialize the game score\n",
    "    game_score = 0\n",
    "    \n",
    "    # Get the state + preprocess the state\n",
    "    state = process_frame(env.reset())\n",
    "    \n",
    "    # Array of images to be blended\n",
    "    images = deque(maxlen=blend)  \n",
    "    \n",
    "    # Append the state to images\n",
    "    images.append(state)\n",
    "    \n",
    "    # Skip the start of each game\n",
    "    for skip in range(skip_start):\n",
    "        env.step(0)\n",
    "    \n",
    "    # Iterate through timesteps\n",
    "    for time in range(20000):\n",
    "        \n",
    "        # Render the environment\n",
    "        env.render()\n",
    "        \n",
    "        # Add one to total_time\n",
    "        total_time += 1\n",
    "        \n",
    "        # Every update_rate timesteps\n",
    "        if total_time % agent.update_rate == 0:\n",
    "            \n",
    "            # Update the target network parameters\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        # Average the last 4 frames\n",
    "        state = blend_images(images, blend)\n",
    "        \n",
    "        # Take action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # One timestep of the environment's dynamics\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Average the last 4 frames\n",
    "        next_state = process_frame(next_state)\n",
    "        images.append(next_state)\n",
    "        next_state = blend_images(images, blend)\n",
    "        \n",
    "        # Store sequence in replay memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update the state\n",
    "        state = next_state\n",
    "        \n",
    "        # Add reward into game_score\n",
    "        game_score += reward\n",
    "        \n",
    "        # Punish behavior which does not accumulate reward\n",
    "        reward -= 1 \n",
    "        \n",
    "        # Append the reward into total_reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # If terminal state\n",
    "        if done:\n",
    "            \n",
    "            # Add game_score into all_rewards\n",
    "            all_rewards += game_score\n",
    "            \n",
    "            # Print episode, reward, average reward, time\n",
    "            print(\"episode: {}/{}, game score: {}, reward: {}, avg reward: {}, time: {}, total time: {}\"\n",
    "                  .format(e+1, episodes, game_score, total_reward, all_rewards/(e+1), time, total_time))\n",
    "            \n",
    "            # Break\n",
    "            break\n",
    "        \n",
    "        # If length of memory gets larger than batch_size\n",
    "        if len(agent.memory) > batch_size:\n",
    "            \n",
    "            # Randomely selecting experiences (in the replay memory) and training on them.\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
