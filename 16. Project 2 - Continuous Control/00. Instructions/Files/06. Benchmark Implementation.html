<!-- udacimak v1.2.1 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Benchmark Implementation</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>Continuous Control</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Unity ML-Agents.html">01. Unity ML-Agents</a>
    </li>
    <li class="">
      <a href="02. The Environment - Introduction.html">02. The Environment - Introduction</a>
    </li>
    <li class="">
      <a href="03. The Environment - Real World.html">03. The Environment - Real World</a>
    </li>
    <li class="">
      <a href="04. The Environment - Explore.html">04. The Environment - Explore</a>
    </li>
    <li class="">
      <a href="05. Project Instructions.html">05. Project Instructions</a>
    </li>
    <li class="">
      <a href="06. Benchmark Implementation.html">06. Benchmark Implementation</a>
    </li>
    <li class="">
      <a href="07. Not sure where to start.html">07. Not sure where to start?</a>
    </li>
    <li class="">
      <a href="08. General Advice.html">08. General Advice</a>
    </li>
    <li class="">
      <a href="09. Collaborate!.html">09. Collaborate!</a>
    </li>
    <li class="">
      <a href="10. Workspace.html">10. Workspace</a>
    </li>
    <li class="">
      <a href="11. (Optional) Challenge Crawl.html">11. (Optional) Challenge: Crawl</a>
    </li>
    <li class="">
      <a href="Project Description - Continuous Control.html">Project Description - Continuous Control</a>
    </li>
    <li class="">
      <a href="Project Rubric - Continuous Control.html">Project Rubric - Continuous Control</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">06. Benchmark Implementation</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h1 id="benchmark-implementation">Benchmark Implementation</h1>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>For this project, you can use any algorithm of your choosing to solve the task.  You are strongly encouraged to do your own research, to devise your own approach towards solving this problem.  </p>
<p>In case you get stuck, here are the details of one approach that worked well for us.  </p>
<h2 id="-an-amended-ddpg-agent">## An Amended DDPG Agent</h2>
<p>In this part of the Nanodegree program, you learned about a lot of potential ways to solve this project.  We instead decided to solve the project by making some amendments to the Deep Deterministic Policy Gradients (DDPG) algorithm.</p>
<h3 id="attempt-1">Attempt 1</h3>
<p>The first thing that we did was amend the DDPG code to work for multiple agents, to solve version 2 of the environment.  The DDPG code in the DRLND GitHub repository utilizes only a single agent, and with each step:</p>
<ul>
<li>the agent adds its experience to the replay buffer, and</li>
<li>the (local) actor and critic networks are updated, using a sample from the replay buffer.</li>
</ul>
<p>So, in order to make the code work with 20 agents, we modified the code so that after each step:</p>
<ul>
<li>each agent adds its experience to a replay buffer that is shared by all agents, and</li>
<li>the (local) actor and critic networks are updated 20 times in a row (one for each agent), using 20 different samples from the replay buffer.</li>
</ul>
<p>In hindsight, this wasn't a great plan, but it was a start!  That said, the scores are shown below.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2018-05-02-at-4.56.45-pm.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>You'll notice that we made some rapid improvement pretty early in training, because of the extremely large number of updates.  Unfortunately, also due to the large number of updates, the agent is incredibly unstable.  Around episode 100, performance crashed and did not recover.</p>
<p>So, we focused on determining ways to stabilize this first attempt.</p>
<h3 id="attempt-2">Attempt 2</h3>
<p>For this second attempt, we reduced the number of agents from 20 to 1 (by switching to version 1 of the environment).  We wanted to know how much stability we could expect from a single agent.  The idea was that the code would likely train more reliably, if we didn't make so many updates.  And it did train much better.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2018-05-03-at-9.10.50-am.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>At one point, we even hit the target score of 30.  However, this score wasn't maintained for very long, and we saw strong indications that the algorithm was going to crash again.  This showed us that we needed to spend more time with figuring out how to stabilize the algorithm, if we wanted to have a chance of training all 20 agents simultaneously.</p>
<h3 id="attempt-3">Attempt 3</h3>
<p>This time, we switched back to version 2 of the environment, and began with the code from <strong>Attempt 1</strong> as a starting point.  Then, the only change we made was to use gradient clipping when training the critic network.  The corresponding snippet of code was as follows:</p>
<pre><code class="python language-python">self.critic_optimizer.zero_grad()
critic_loss.backward()
torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)
self.critic_optimizer.step()</code></pre>
<p>The corresponding scores are plotted below.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/pic3.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>This is when we really started to feel hopeful.  We still didn't maintain an average score of 30 over 100 episodes, but we maintained the score for longer than before.  And the agent didn't crash as suddenly as in the previous attempts!</p>
<h3 id="attempt-4">Attempt 4</h3>
<p>At this point, we decided to get less aggressive with the number of updates per time step.  In particular, instead of updating the actor and critic networks <strong>20 times</strong> at <strong>every timestep</strong>, we amended the code to update the networks <strong>10 times</strong> after every <strong>20 timesteps</strong>.  The corresponding scores are plotted below.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/unknown.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>And, this was enough to solve the environment!  In hindsight, we probably should have realized this fix much earlier, but this long path to the solution was definitely a nice way to help with building intuition! :)</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-note">## Note</h2>
<p>If you are interested in implementing a method that will be more stable with the project, please explore <a href="https://arxiv.org/abs/1604.06778" target="_blank">this paper</a>.  As discussed in the paper, Trust Region Policy Optimization (TRPO) and Truncated Natural Policy Gradient (TNPG) should achieve better performance.  You may also like to write your own implementation of Proximal Policy Optimization (PPO), which has also <a href="https://blog.openai.com/openai-baselines-ppo/" target="_blank">demonstrated good performance</a> with continuous control tasks.</p>
<p>You may also like to explore the (very!) recent <a href="https://openreview.net/forum?id=SyZipzbCb" target="_blank">Distributed Distributional Deterministic Policy Gradients (D4PG)</a> algorithm as another method for adapting DDPG for continuous control.</p>
</div>

</div>
<div class="divider"></div>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://github.com/udacimak/udacimak#readme" target="_blank">udacimak v1.2.1</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });
    });
  </script>
</body>

</html>
