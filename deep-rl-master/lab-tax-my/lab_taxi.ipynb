{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab-taxi.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linked0/deep-rl/blob/master/lab_taxi/lab_taxi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yqhTr85IGjam",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 개요\n",
        "- 각 TD method로 구현해보기\n",
        "\n",
        "### 검증방법\n",
        "- 샘플 자료와 동일한 값이 나오는 지 확인 (아래 '참고 구현' 참고)\n",
        "\n",
        "\n",
        "### 참고 자료\n",
        "##### Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\n",
        "https://arxiv.org/pdf/cs/9905014.pdf\n",
        "\n",
        "##### OpenAI Gym environment\n",
        "https://github.com/openai/gym/tree/master/gym\n",
        "\n",
        "##### 참고 구현\n",
        "https://github.com/AdalbertoCq/Deep-Learning-Nanodegree-Udacity/blob/master/Reinforcement%20Learning/taxi-v2/Agent.py\n"
      ]
    },
    {
      "metadata": {
        "id": "Iww18rMT0T9b",
        "colab_type": "code",
        "outputId": "1c0d73a8-6dff-4c7c-f58f-dc3736646f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ux3ffAOh0B6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from six import StringIO\n",
        "from gym import utils\n",
        "from gym.envs.toy_text import discrete\n",
        "import numpy as np\n",
        "\n",
        "MAP = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : : : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "class TaxiEnv(discrete.DiscreteEnv):\n",
        "    \"\"\"\n",
        "    The Taxi Problem\n",
        "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
        "    by Tom Dietterich\n",
        "    Description:\n",
        "    There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drive to the passenger's location, pick up the passenger, drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. Once the passenger is dropped off, the episode ends.\n",
        "    Observations: \n",
        "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is the taxi), and 4 destination locations. \n",
        "    \n",
        "    Actions: \n",
        "    There are 6 discrete deterministic actions:\n",
        "    - 0: move south\n",
        "    - 1: move north\n",
        "    - 2: move east \n",
        "    - 3: move west \n",
        "    - 4: pickup passenger\n",
        "    - 5: dropoff passenger\n",
        "    \n",
        "    Rewards: \n",
        "    There is a reward of -1 for each action and an additional reward of +20 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
        "    \n",
        "    Rendering:\n",
        "    - blue: passenger\n",
        "    - magenta: destination\n",
        "    - yellow: empty taxi\n",
        "    - green: full taxi\n",
        "    - other letters: locations\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human', 'ansi']}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.desc = np.asarray(MAP,dtype='c')\n",
        "\n",
        "        self.locs = locs = [(0,0), (0,4), (4,0), (4,3)]\n",
        "\n",
        "        nS = 500\n",
        "        nR = 5\n",
        "        nC = 5\n",
        "        maxR = nR-1\n",
        "        maxC = nC-1\n",
        "        isd = np.zeros(nS)\n",
        "        nA = 6\n",
        "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "                for passidx in range(5):\n",
        "                    for destidx in range(4):\n",
        "                        state = self.encode(row, col, passidx, destidx)\n",
        "                        if passidx < 4 and passidx != destidx:\n",
        "                            isd[state] += 1\n",
        "                        for a in range(nA):\n",
        "                            # defaults\n",
        "                            newrow, newcol, newpassidx = row, col, passidx\n",
        "                            reward = -1\n",
        "                            done = False\n",
        "                            taxiloc = (row, col)\n",
        "\n",
        "                            if a==0:\n",
        "                                newrow = min(row+1, maxR)\n",
        "                            elif a==1:\n",
        "                                newrow = max(row-1, 0)\n",
        "                            if a==2 and self.desc[1+row,2*col+2]==b\":\":\n",
        "                                newcol = min(col+1, maxC)\n",
        "                            elif a==3 and self.desc[1+row,2*col]==b\":\":\n",
        "                                newcol = max(col-1, 0)\n",
        "                            elif a==4: # pickup\n",
        "                                if (passidx < 4 and taxiloc == locs[passidx]):\n",
        "                                    newpassidx = 4\n",
        "                                else:\n",
        "                                    reward = -10\n",
        "                            elif a==5: # dropoff\n",
        "                                if (taxiloc == locs[destidx]) and passidx==4:\n",
        "                                    newpassidx = destidx\n",
        "                                    done = True\n",
        "                                    reward = 20\n",
        "                                elif (taxiloc in locs) and passidx==4:\n",
        "                                    newpassidx = locs.index(taxiloc)\n",
        "                                else:\n",
        "                                    reward = -10\n",
        "                            newstate = self.encode(newrow, newcol, newpassidx, destidx)\n",
        "                            P[state][a].append((1.0, newstate, reward, done))\n",
        "        isd /= isd.sum()\n",
        "        discrete.DiscreteEnv.__init__(self, nS, nA, P, isd)\n",
        "\n",
        "    def encode(self, taxirow, taxicol, passloc, destidx):\n",
        "        # (5) 5, 5, 4\n",
        "        i = taxirow\n",
        "        i *= 5\n",
        "        i += taxicol\n",
        "        i *= 5\n",
        "        i += passloc\n",
        "        i *= 4\n",
        "        i += destidx\n",
        "        return i\n",
        "\n",
        "    def decode(self, i):\n",
        "        out = []\n",
        "        out.append(i % 4)\n",
        "        i = i // 4\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i)\n",
        "        assert 0 <= i < 5\n",
        "        return reversed(out)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        taxirow, taxicol, passidx, destidx = self.decode(self.s)\n",
        "        def ul(x): return \"_\" if x == \" \" else x\n",
        "        if passidx < 4:\n",
        "            out[1+taxirow][2*taxicol+1] = utils.colorize(out[1+taxirow][2*taxicol+1], 'yellow', highlight=True)\n",
        "            pi, pj = self.locs[passidx]\n",
        "            out[1+pi][2*pj+1] = utils.colorize(out[1+pi][2*pj+1], 'blue', bold=True)\n",
        "        else: # passenger in taxi\n",
        "            out[1+taxirow][2*taxicol+1] = utils.colorize(ul(out[1+taxirow][2*taxicol+1]), 'green', highlight=True)\n",
        "\n",
        "        di, dj = self.locs[destidx]\n",
        "        out[1+di][2*dj+1] = utils.colorize(out[1+di][2*dj+1], 'magenta')\n",
        "        outfile.write(\"\\n\".join([\"\".join(row) for row in out])+\"\\n\")\n",
        "        if self.lastaction is not None:\n",
        "            outfile.write(\"  ({})\\n\".format([\"South\", \"North\", \"East\", \"West\", \"Pickup\", \"Dropoff\"][self.lastaction]))\n",
        "        else: outfile.write(\"\\n\")\n",
        "\n",
        "        # No need to return anything for human\n",
        "        if mode != 'human':\n",
        "            return outfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPyc_zWYVFcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\" Initialize agent.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "        self.alpha = 0.20\n",
        "        self.gamma = 1\n",
        "        self.td_method = 0 # 0:Sarsa, 1:Q-Learning, 2:Sarsa expected\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        \"\"\" Given the state, select an action.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"\n",
        "        # Implementation for e-greedy policy.\n",
        "        action_max = np.argmax(self.Q[state])\n",
        "        probabilities = np.ones(self.nA) * epsilon / self.nA\n",
        "        probabilities[action_max] = 1 - epsilon + epsilon / self.nA\n",
        "        return np.random.choice(a=np.arange(self.nA), p=probabilities)\n",
        "      \n",
        "    def expected_reward(self, next_state, epsilon):\n",
        "      probabilities = np.ones(self.nA) * epsilon / self.na\n",
        "      action_max = np.argmax(self.Q[next_state])\n",
        "      probabilities[action_max] = 1 - epsilon + epsilon / self.nA\n",
        "      expected = 0\n",
        "      for action, prob in enumerate(probabilities):\n",
        "        expected += prob * self.Q[next_state][action]\n",
        "      return expected\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done, epsilon):\n",
        "        \"\"\" Update the agent's knowledge, using the most recently sampled tuple.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "        if self.td_method == 0: # Sarsa\n",
        "          next_action = self.select_action(next_state, epsilon)\n",
        "          self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + self.gamma*self.Q[next_state][next_action] - self.Q[state][action])\n",
        "        elif self.td_method == 1: # Q-Learning\n",
        "          self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + self.gamma*np.max(self.Q[next_state]) - self.Q[state][action])\n",
        "        else: # Expected Sarsa\n",
        "          self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + self.gamma*expected_rweard(next_state, epsilon) - self.Q[state][action])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xl_OTSQPVSt2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\" Monitor agent's performance.\n",
        "    \n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of OpenAI Gym's Taxi-v1 environment\n",
        "    - agent: instance of class Agent (see Agent.py for details)\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "        # begin the episode\n",
        "        state = env.reset()\n",
        "        # initialize the sampled reward\n",
        "        samp_reward = 0\n",
        "        epsilon = 1.0 / i_episode\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state, epsilon)\n",
        "            # agent performs the selected action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            agent.step(state, action, reward, next_state, done, epsilon)\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state (s <- s') to next time step\n",
        "            state = next_state\n",
        "            if done:\n",
        "                # save final sampled reward\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "        if (i_episode >= 100):\n",
        "            # get average reward from last 100 episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            # append to deque\n",
        "            avg_rewards.append(avg_reward)\n",
        "            # update best average reward\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "        # monitor progress\n",
        "        print(\"\\rEpisode {}/{} || Best average reward {}\".format(i_episode, num_episodes, best_avg_reward), end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        # check if task is solved (according to OpenAI Gym)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print('\\nEnvironment solved in {} episodes.'.format(i_episode), end=\"\")\n",
        "            break\n",
        "        if i_episode == num_episodes: print('\\n')\n",
        "    return avg_rewards, best_avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7mI0zqv_WMpZ",
        "colab_type": "code",
        "outputId": "2fa4092f-168c-4207-c421-b646c7b2282b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "#from agent import Agent\n",
        "#from monitor import interact\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('Taxi-v2')\n",
        "agent = Agent()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uhu2BTLXVVOz",
        "colab_type": "code",
        "outputId": "b00cbd32-e0e8-4992-8e5f-a2d00873b82e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "avg_rewards, best_avg_reward = interact(env , agent)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 9.22\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}