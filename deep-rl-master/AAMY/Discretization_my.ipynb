{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/linked0/deep-rl/blob/master/discretization/Discretization_my.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정리\n",
    "- 이 코드는 openai gym 깃헙 사이트에서 gym 폴더만을 포함시켜서 테스트함.\n",
    "- 로컬 피씨에서 jupyter notebook으로 실행시켜야 동작 UI를 확인할 수 있음.\n",
    "- gym/envs/robotics 폴더는 사이즈때문에 지움."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pbl_xA3banb"
   },
   "source": [
    "# Discretization\n",
    "---\n",
    "In this notebook, you will deal with continuous state and action spaces by discretizing them. This will enable you to apply reinforcement learning algorithms that are only designed to work with discrete spaces.\n",
    "\n",
    "### 1. Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KhrVYHj4bX9F"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KeqGfkACcYgT"
   },
   "source": [
    "### 2. Specify the Environment, and Explore the State and Action Spaces\n",
    "We'll use [OpenAI Gym](https://gym.openai.com/) environments to test and develop our algorithms. These simulate a variety of classic as well as contemporary reinforcement learning tasks. Let's use an environment that has a continuous state space, but a discrete action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "5Dzx3XOEcWRr",
    "outputId": "994663b4-4057-40b6-b9d6-72d1d6ff7434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "low:[-1.2  -0.07], high:[ 0.6   0.07] in Box\n"
     ]
    }
   ],
   "source": [
    "# Create an environment and set random seed\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(505);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-Yerz78dLoT"
   },
   "source": [
    "Run the next code cell to watch a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 915
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "p8ri5M6-dyV8",
    "outputId": "598600a3-68d5-46a7-83b6-72394cf4f121"
   },
   "outputs": [],
   "source": [
    "# Install Chainer, ChainerRL and CuPy!\n",
    "import matplotlib\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "id": "s05uoI5SdK9E",
    "outputId": "5ee6b989-60f7-4b4d-81d3-022c836dacb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score: -200.0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "score = 0\n",
    "for t in range(200):\n",
    "  action = env.action_space.sample()\n",
    "  env.render()\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  score += reward\n",
    "  if done:\n",
    "    break\n",
    "print('Final score:', score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will train an agent to perform much better! For now, we can explore the state and action spaces, as well as sample them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bE8U5Ohxdp_2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box(2,)\n",
      "- low: [-1.2  -0.07]\n",
      "- high: [ 0.6   0.07]\n"
     ]
    }
   ],
   "source": [
    "# Explore state (observation) space\n",
    "print('State space:', env.observation_space)\n",
    "print('- low:', env.observation_space.low)\n",
    "print('- high:', env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space samples\n",
      "[[-0.622 -0.039]\n",
      " [-0.946 -0.056]\n",
      " [ 0.571 -0.034]\n",
      " [-0.233 -0.007]\n",
      " [-1.021 -0.021]\n",
      " [-0.355  0.048]\n",
      " [ 0.428 -0.065]\n",
      " [-0.285 -0.047]\n",
      " [ 0.202  0.051]\n",
      " [-0.459 -0.05 ]]\n"
     ]
    }
   ],
   "source": [
    "# Generate some samples from the state space\n",
    "print(\"State space samples\")\n",
    "print(np.array([env.observation_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "Action space samples:\n",
      "[1 1 1 2 2 2 0 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Explore the actio space\n",
    "print('Action space:', env.action_space)\n",
    "\n",
    "# Generate some samples from the action space\n",
    "print('Action space samples:')\n",
    "print(np.array([env.action_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discetize the State Space with a Uniform Grid\n",
    "We will discretize the space using a uniformly-spaced grid. Implement the following function to create such a grid, give the lower bounds(low), upper bounds(high), and number of desired bins along each dimension. It should return the split points for each demension, which will be 1 less than the number of bins.\n",
    "\n",
    "For instance, if `(low = [-1.0, -0.5]`, `high = [1.0, 5.0]`, and `bins = (10, 10)`, then your function should return the following list of 2 Numpy arrays:\n",
    "```\n",
    "[array([-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]),\n",
    "array([-4.0, -3.0, -2.0. -1.0, 0.0, 1.0, 2.0, 3.0, 4.0])]\n",
    "```\n",
    "Nothe that the ends of `low` and `high` are **not** included in these split points. It is assumed that any value below the lowest split point maps index `0` and any value above the highest split point maps to index `n-1`, where `n` is the number of bins along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8]),\n",
       " array([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_uniform_grid(low, high, bins=(10,10)):\n",
    "  \"\"\" Define a uniformly-spaced grid that can be used to discretize a space.\n",
    "  Parameters\n",
    "  ----------\n",
    "  low: array_like\n",
    "      Lower bounds for each dimension of the continuous space.\n",
    "  high: array_like\n",
    "      Upper bounds for each dimension of the continuous space.\n",
    "  bins: tuple\n",
    "      Number of bins along each corresponding dimension.\n",
    "  Returns\n",
    "  -------\n",
    "  grid: list of array_like\n",
    "      A list of arrays containing split points for each dimension.\n",
    "  \"\"\"\n",
    "  grid = [np.linspace(val[0], val[1], val[2] + 1)[1:-1] for val in zip(low, high, bins)]\n",
    "  return grid\n",
    "\n",
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]\n",
    "create_uniform_grid(low, high) # [test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Discretization_my.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
